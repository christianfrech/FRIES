/*! \file
 *
 * \brief Utilities for storing and manipulating sparse vectors
 *
 * Supports sparse vectors distributed among multiple processes if USE_MPI is
 * defined
 */

#ifndef vec_utils_h
#define vec_utils_h

#include <stdio.h>
#include <string.h>
#include <math.h>
#include <FRIES/det_store.h>
#include <FRIES/Hamiltonians/hub_holstein.hpp>
#include <FRIES/Ext_Libs/dcmt/dc.h>
#include <FRIES/mpi_switch.h>
#include <FRIES/ndarr.hpp>
#include <vector>

using namespace std;


#ifdef USE_MPI
#define CONV_MPI(type)(type ? MPI_INT : MPI_DOUBLE)
template<typename T>
struct mpi_type
{
};

// template specialization
template<>
struct mpi_type<double>
{
    static constexpr int value = 0;
};

template<>
struct mpi_type<int>
{
    static constexpr int value = 1;
};

#endif

/*! \brief Generate list of occupied orbitals from bit-string representation of
 *  a determinant
 *
 * This implementation uses the procedure in Sec. 3.1 of Booth et al. (2014)
 * \param [in] det          bit string to be parsed
 * \param [in] table        byte table structure generated by gen_byte_table()
 * \param [out] occ_orbs    Occupied orbitals in the determinant
 * \return number of 1 bits in the bit string
 */
unsigned char gen_orb_list(long long det, byte_table *table, unsigned char *occ_orbs);


/*! \brief Generate lists of occupied orbitals in a determinant that are
 *  adjacent to an empty orbital if the orbitals represent sites on a 1-D lattice
 *
 * \param [in] det          bit string representation of the determinant
 * \param [in] n_sites      number of sites in the lattice
 * \param [in] table        byte table struct generated by gen_byte_table()
 * \param [in] n_elec       number of electrons in the system
 * \param [out] neighbors   2-D array whose 0th row indicates orbitals with an
 *                          empty adjacent orbital to the left, and 1st row is
 *                          for empty orbitals to the right. Elements in the 0th
 *                          column indicate number of elements in each row.
 */
void find_neighbors_1D(long long det, unsigned int n_sites, byte_table *table,
                       unsigned int n_elec, unsigned char *neighbors);

template <class el_type>
class DistVec;

/*!
 * \brief Class for adding elements to a DistVec object
 * \tparam el_type Type of elements to be added to the DistVec object
 * Elements are first added to a buffer, and then the buffered elements can be distributed to the appropriate process by calling perform_add()
 */
template <class el_type>
class Adder {
public:
    /*! \brief Constructor for Adder class
     * Allocates memory for the internal buffers in the class
     * \param [in] size     Maximum number of elements per MPI process in send and receive buffers
     * \param [in] n_procs  The number of processes
     * \param [in] vec         The vector to which elements will be added
     */
    Adder(size_t size, int n_procs, DistVec<el_type> *vec) : send_idx_(n_procs, size), send_vals_(n_procs, size), recv_idx_(n_procs, size), recv_vals_(n_procs, size), parent_vec_(vec) {
        send_cts_ = (int *)malloc(sizeof(int) * n_procs);
        recv_cts_ = (int *) malloc(sizeof(int) * n_procs);
        displacements_ = (int *) malloc(sizeof(int) * n_procs);
        int proc_idx;
        for (proc_idx = 0; proc_idx < n_procs; proc_idx++) {
            displacements_[proc_idx] = proc_idx * (int)size;
            send_cts_[proc_idx] = 0;
        }
    }
    
    ~Adder() {
        free(send_cts_);
        free(recv_cts_);
        free(displacements_);
    }
    
    Adder(const Adder &a) = delete;
    
    Adder& operator= (const Adder &a) = delete;
    
    /*! \brief Remove the elements from the internal buffers and send them to the DistVec objects on their corresponding MPI processes
    * \param [in] ini_bit  The bit in indices used to determine whether each added element corresponds to an initiator
     */
    void perform_add(long long ini_bit);
    
    /*! \brief Add an element to the internal buffers
     * \param [in] idx      Index of the element to be added
     * \param [in] val      Value of the added element
     * \param [in] ini_flag     A bit string indicating the initiator status of the added element
     */
    void add(long long idx, el_type val, long long ini_flag);
private:
    Matrix<long long> send_idx_; ///< Send buffer for element indices
    Matrix<el_type> send_vals_; ///< Send buffer for element values
    Matrix<long long> recv_idx_; ///< Receive buffer for element indices
    Matrix<el_type> recv_vals_; ///< Receive buffer for element values
    int *send_cts_; ///< Number of elements in the send buffer for each process
    int *recv_cts_; ///< Number of elements in the receive buffer for each process
    int *displacements_; ///< Array positions in buffers corresponding to each
    DistVec<el_type> *parent_vec_; ///<The DistVec object to which elements are added
#ifdef USE_MPI
    static constexpr int my_mpi_type = mpi_type<el_type>::value;
#endif
    
/*! \brief Increase the size of the buffer for temporarily storing added elements
 */
    void enlarge_() {
        printf("Increasing storage capacity in adder\n");
        size_t n_proc = send_idx_.rows();
        size_t new_cols = send_idx_.cols() * 2;
        
        send_idx_.enlarge_cols(new_cols, send_cts_);
        send_vals_.enlarge_cols(new_cols, send_cts_);
        recv_idx_.reshape(n_proc, new_cols);
        recv_vals_.reshape(n_proc, new_cols);
        
        int proc_idx;
        for (proc_idx = 0; proc_idx < n_proc; proc_idx++) {
            displacements_[proc_idx] = proc_idx * (int)new_cols;
        }
    }
};

/*!
 * \brief Class for storing and manipulating a sparse vector
 * \tparam el_type Type of elements in the vector
 * Elements of the vector are distributed across many MPI processes, and hashing is used for efficient indexing
 */
template <class el_type>
class DistVec {
    long long *indices_; ///< Array of indices of vector elements
    std::vector<el_type> values_; ///< Array of values of vector elements
    double *matr_el_; ///< Array of pre-calculated diagonal matrix elements associated with each vector element
    size_t max_size_; ///< Maximum number of vector elements that can be stored
    size_t curr_size_; ///< Current number of vector elements stored, including intermediate zeroes
    hash_table *vec_hash_; ///< Hash table for quickly finding indices in \p indices_
    stack_entry *vec_stack_; ///< Pointer to top of stack for managing available positions in the indices array
    byte_table *tabl_; ///< Pointer to struct used to decompose determinant indices into lists of occupied orbitals
    Matrix<unsigned char> occ_orbs_; ///< Matrix containing lists of occupied orbitals for each determniant index
    Matrix<unsigned char> neighb_; ///< Pointer to array containing information about empty neighboring orbitals for Hubbard model
    unsigned int n_sites_; ///< Number of sites along one dimension of the Hubbard lattice, if applicable
    Adder<el_type> adder_; ///< Pointer to adder struct for buffered addition of elements distributed across MPI processes
    int n_nonz_; /// Current number of nonzero elements in vector
#ifdef USE_MPI
    static constexpr int my_mpi_type = mpi_type<el_type>::value;
#endif
public:
    unsigned int *proc_scrambler_; ///< Array of random numbers used in the hash function for assigning vector indices to MPI
    
    /*! \brief Constructor for DistVec object
    * \param [in] size         Maximum number of elements to be stored in the vector
    * \param [in] add_size     Maximum number of elements per processor to use in Adder object
    * \param [in] rn_ptr       Pointer to an mt_struct object for RN generation
    * \param [in] n_orb        Number of spatial orbitals in the basis (half the length of the vector of random numbers for the
    *                          hash function for processors)
    * \param [in] n_elec       Number of electrons represented in each vector index
    * \param [in] n_sites      If the orbitals in vector indices represent lattice sites, the number of sites along one dimension. 0 otherwise.
     * \param [in] n_procs Number of MPI processes over which to distribute vector elements
     */
    DistVec(size_t size, size_t add_size, mt_struct *rn_ptr, unsigned int n_orb,
            unsigned int n_elec, int n_sites, int n_procs) : n_sites_(n_sites), values_(size), max_size_(size), curr_size_(0), vec_stack_(NULL), occ_orbs_(size, n_elec), adder_(add_size, n_procs, this), n_nonz_(0), neighb_(n_sites ? size : 0, 2 * (n_elec + 1)) {
        indices_ = (long long *) malloc(sizeof(long long) * size);
        matr_el_ = (double *)malloc(sizeof(double) * size);
        vec_hash_ = setup_ht(size, rn_ptr, 2 * n_orb);
        tabl_ = gen_byte_table();
    }
    
    ~DistVec() {
        free(indices_);
        free(matr_el_);
    }
    
    DistVec(const DistVec &d) = delete;
    
    DistVec& operator= (const DistVec& d) = delete;

    /*! \brief Calculate dot product
     *
     * Calculates dot product of the portion of a DistVec object stored on each MPI process
     * with a local sparse vector (such that the local results could be added)
     *
     * \param [in] idx2         Indices of elements in the local vector
     * \param [in] vals2         Values of elements in the local vector
     * \param [in] num2         Number of elements in the local vector
     * \param [in] hashes2      hash values of the indices of the local vector from
     *                          the hash table of vec
     * \return the value of the dot product
     */
    double dot(long long *idx2, double *vals2, size_t num2,
               unsigned long long *hashes2) {
        size_t hf_idx;
        ssize_t *ht_ptr;
        double numer = 0;
        for (hf_idx = 0; hf_idx < num2; hf_idx++) {
            ht_ptr = read_ht(vec_hash_, idx2[hf_idx], hashes2[hf_idx], 0);
            if (ht_ptr) {
                numer += vals2[hf_idx] * values_[*ht_ptr];
            }
        }
        return numer;
    }
    
    /*! \brief Double the maximum number of elements that can be stored */
    void expand() {
        printf("Increasing storage capacity in vector\n");
        size_t new_max = max_size_ * 2;
        indices_ = (long long *)realloc(indices_, sizeof(long long) * new_max);
        matr_el_ = (double *)realloc(matr_el_, sizeof(double) * new_max);
        occ_orbs_.reshape(new_max, occ_orbs_.cols());
        if (n_sites_) {
            neighb_.reshape(new_max, neighb_.cols());
        }
        values_.resize(new_max);
        max_size_ = new_max;
    }

    /*! \brief Hash function mapping vector index to MPI process
     *
     * \param [in] idx          Vector index
     * \return process index from hash value
     */
    int idx_to_proc(long long idx) {
        unsigned int n_elec = (unsigned int)occ_orbs_.cols();
        unsigned char orbs[n_elec];
        gen_orb_list(idx, tabl_, orbs);
        unsigned long long hash_val = hash_fxn(orbs, n_elec, proc_scrambler_);
        int n_procs = 1;
#ifdef USE_MPI
        MPI_Comm_size(MPI_COMM_WORLD, &n_procs);
#endif
        return hash_val % n_procs;
    }

    /*! \brief Hash function mapping vector index to local hash value
     *
     * The local hash value is used to find the index on a particular processor
     *
     * \param [in] idx          Vector index
     * \return hash value
     */
    unsigned long long idx_to_hash(long long idx) {
        unsigned int n_elec = (unsigned int)occ_orbs_.cols();
        unsigned char orbs[n_elec];
        gen_orb_list(idx, tabl_, orbs);
        return hash_fxn(orbs, n_elec, vec_hash_->scrambler);
    }

    /*! \brief Add an element to the DistVec object
     *
     * The element will be added to a buffer for later processing
     *
     * \param [in] idx          The index of the element in the vector
     * \param [in] val          The value of the added element
     * \param [in] ini_flag     A bit string indicating whether the added element
     *                          came from an initiator element. None of the 1 bits
     *                          should overlap with the orbitals encoded in the rest
     *                          of the bit string
     */
    void add(long long idx, el_type val, long long ini_flag) {
        if (val != 0) {
            adder_.add(idx, val, ini_flag);
        }
    }

    /*! \brief Incorporate elements from the Adder buffer into the vector
     *
     * Sign-coherent elements are added regardless of their corresponding initiator
     * flags. Otherwise, only elements with nonzero initiator flags are added
     *
     * \param [in] ini_bit      A bit mask defining where to look for initiator
     *                          flags in added elements
     */
    void perform_add(long long ini_bit) {
        adder_.perform_add(ini_bit);
    }
    
    /*! \brief Get the index of an unused intermediate index in the \p indices_ array, or -1 if none exists */
    ssize_t pop_stack() {
        stack_entry *head = vec_stack_;
        if (!head) {
            return -1;
        }
        ssize_t ret_idx = head->idx;
        vec_stack_ = head->next;
        free(head);
        return ret_idx;
    }
    
    /*! \brief Push an unused index in the \p indices_ array onto the stack
     * \param [in] idx The index of the available element of the \p indices array
     */
    void push_stack(size_t idx) {
        stack_entry *new_entry = (stack_entry*) malloc(sizeof(stack_entry));
        new_entry->idx = idx;
        new_entry->next = vec_stack_;
        vec_stack_ = new_entry;
    }

    /*! \brief Delete an element from the vector
     *
     * Removes an element from the vector and modifies the hash table accordingly
     *
     * \param [in] pos          The position of the element to be deleted in \p indices_
     */
    void del_at_pos(size_t pos) {
        long long idx = indices_[pos];
        unsigned long long hash_val = idx_to_hash(idx);
        push_stack(pos);
        del_ht(vec_hash_, idx, hash_val);
        n_nonz_--;
    }
    
    /*! \returns The array used to store indices in the DistVec object */
    long long *indices() const {
        return indices_;
    }
    
    /*! \returns The array used to store values in the DistVec object */
    void *values() const {
        return (void *)values_.data();
    }
    
    /*! \returns The current number of elements in use in the \p indices_ and \p values arrays */
    size_t curr_size() const {
        return curr_size_;
    }
    
    /*!\returns The current number of nonzero elements in the vector */
    int n_nonz() const {
        return n_nonz_;
    }
    
    /*! \returns A pointer to the byte_table struct used to perform bit manipulations for this vector */
    byte_table *tabl() const {
        return tabl_;
    }
    
    /*! \returns A reference to the Matrix used to store information about empty neighboring orbitals of
     *              seach determinant in the Hubbard model*/
    const Matrix<unsigned char> &neighb() const{
        return neighb_;
    }
    
    /*! \brief Add elements destined for this process to the DistVec object
     * \param [in] indices Indices of the elements to be added
     * \param [in] vals     Values of the elements to be added
     * \param [in] count    Number of elements to be added
     * \param [in] ini_bit  The bit in indices used to determine whether each added element corresponds to an initiator
     */
    void add_elements(long long *indices, el_type *vals, size_t count, long long ini_bit) {
        size_t el_idx;
        unsigned int n_elec = (unsigned int)occ_orbs_.cols();
        for (el_idx = 0; el_idx < count; el_idx++) {
            long long new_idx = indices[el_idx];
            int ini_flag = !(!(new_idx & ini_bit));
            new_idx &= ini_bit - 1;
            unsigned long long hash_val = idx_to_hash(new_idx);
            ssize_t *idx_ptr = read_ht(vec_hash_, new_idx, hash_val, ini_flag);
            if (idx_ptr && *idx_ptr == -1) {
                *idx_ptr = pop_stack();
                if (*idx_ptr == -1) {
                    if (curr_size_ >= max_size_) {
                        expand();
                    }
                    *idx_ptr = curr_size_;
                    curr_size_++;
                }
                values_[*idx_ptr] = 0;
                if (gen_orb_list(new_idx, tabl_, occ_orbs_[*idx_ptr]) != n_elec) {
                    fprintf(stderr, "Error: determinant %lld created with an incorrect number of electrons.\n", new_idx);
                }
                indices_[*idx_ptr] = new_idx;
                matr_el_[*idx_ptr] = NAN;
                n_nonz_++;
                if (n_sites_) {
                    find_neighbors_1D(new_idx, n_sites_, tabl_, n_elec, neighb_[*idx_ptr]);
                }
            }
            int del_bool = 0;
            if (ini_flag || (idx_ptr && (values_[*idx_ptr] * vals[el_idx]) > 0)) {
                values_[*idx_ptr] += vals[el_idx];
                del_bool = values_[*idx_ptr] == 0;
            }
            if (del_bool == 1) {
                push_stack(*idx_ptr);
                del_ht(vec_hash_, new_idx, hash_val);
                n_nonz_--;
            }
        }
    }
    
    /*! \brief Get a pointer to a value in the \p values_ array of the DistVec object
     
    * \param [in] pos          The position of the corresponding index in the \p indices_ array
     */
    el_type *operator[](size_t pos) {
        return &values_[pos];
    }

    /*! \brief Get a pointer to the list of occupied orbitals corresponding to an
     * existing determinant index in the vector
     *
     * \param [in] pos          The position of the index in the \p indices_ array
     */
    unsigned char *orbs_at_pos(size_t pos) {
        return occ_orbs_[pos];
    }
    
    /*! \brief Get a pointer to the diagonal matrix element corresponding to an element in the DistVec object
     
    * \param [in] pos          The position of the corresponding index in the \p indices_ array
     */
    double *matr_el_at_pos(size_t pos) {
        return &matr_el_[pos];
    }

    /*! \brief Calculate the sum of the magnitudes of the vector elements on each MPI process
     *
     * \return The sum of the magnitudes on each process
     */
    double local_norm() {
        double norm = 0;
        size_t idx;
        for (idx = 0; idx < curr_size_; idx++) {
            norm += fabs(values_[idx]);
        }
        return norm;
    }

    /*! Save a DistVec object to disk in binary format
     *
     * The vector indices from each MPI process are stored in the file
     * [path]dets[MPI rank].dat, and the values at [path]vals[MPI rank].dat
     *
     * \param [in] path         Location where the files are to be stored
     */
    void save(const char *path)  {
        int my_rank = 0;
    #ifdef USE_MPI
        MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    #endif
        
        size_t el_size = sizeof(el_type);
        
        char buffer[100];
        sprintf(buffer, "%sdets%d.dat", path, my_rank);
        FILE *file_p = fopen(buffer, "wb");
        fwrite(indices_, sizeof(long long), curr_size_, file_p);
        fclose(file_p);
        
        sprintf(buffer, "%svals%d.dat", path, my_rank);
        file_p = fopen(buffer, "wb");
        fwrite(values_.data(), el_size, curr_size_, file_p);
        fclose(file_p);
    }

    /*! Load a vector from disk in binary format
     *
     * The vector indices from each MPI process are read from the file
     * [path]dets[MPI rank].dat, and the values from [path]vals[MPI rank].dat
     *
     * \param [in] path         Location where the files are to be stored
     */
    void load(const char *path) {
        int my_rank = 0;
    #ifdef USE_MPI
        MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    #endif
        
        size_t el_size = sizeof(el_type);
        
        size_t n_dets;
        char buffer[100];
        sprintf(buffer, "%sdets%d.dat", path, my_rank);
        FILE *file_p = fopen(buffer, "rb");
        if (!file_p) {
            fprintf(stderr, "Error: could not open saved binary vector file at %s\n", buffer);
            return;
        }
        n_dets = fread(indices_, sizeof(long long), 10000000, file_p);
        fclose(file_p);
        
        sprintf(buffer, "%svals%d.dat", path, my_rank);
        file_p = fopen(buffer, "rb");
        if (!file_p) {
            fprintf(stderr, "Error: could not open saved binary vector file at %s\n", buffer);
            return;
        }
        fread(values_.data(), el_size, n_dets, file_p);
        fclose(file_p);
        
        size_t det_idx;
        n_nonz_ = 0;
        unsigned int n_elec = (unsigned int)occ_orbs_.cols();
        for (det_idx = 0; det_idx < n_dets; det_idx++) {
            int is_nonz = 0;
            if (fabs(values_[det_idx]) > 1e-9) {
                is_nonz = 1;
                values_[n_nonz_] = values_[det_idx];
            }
            if (is_nonz) {
                gen_orb_list(indices_[det_idx], tabl_, occ_orbs_[n_nonz_]);
                indices_[n_nonz_] = indices_[det_idx];
                matr_el_[n_nonz_] = NAN;
                n_nonz_++;
                if (n_sites_) {
                    find_neighbors_1D(indices_[det_idx], n_sites_, tabl_, n_elec, neighb_[det_idx]);
                }
            }
        }
        curr_size_ = n_nonz_;
    }
    
    /*! \brief Collect all of the vector elements from other MPI processes and accumulate them in the vector on each process */
    void collect_procs() {
        int n_procs = 1;
        int proc_idx;
        int my_rank = 0;
    #ifdef USE_MPI
        MPI_Comm_size(MPI_COMM_WORLD, &n_procs);
        MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    #endif
        int vec_sizes[n_procs];
        vec_sizes[my_rank] = (int)curr_size_;
    #ifdef USE_MPI
        MPI_Allgather(MPI_IN_PLACE, 0, MPI_INT, vec_sizes, 1, MPI_INT, MPI_COMM_WORLD);
        MPI_Datatype mpi_type;
    #endif
        int tot_size = 0;
        int disps[n_procs];
        for (proc_idx = 0; proc_idx < n_procs; proc_idx++) {
            disps[proc_idx] = tot_size;
            tot_size += vec_sizes[proc_idx];
        }
        size_t el_size = sizeof(el_type);
        if (tot_size > max_size_) {
            indices_ = (long long *)realloc(indices_, sizeof(long long) * tot_size);
            values_.resize(tot_size);
        }
        memmove(&indices_[disps[my_rank]], indices_, vec_sizes[my_rank] * sizeof(long long));
        memmove(&values_.data()[disps[my_rank]], values_.data(), vec_sizes[my_rank] * el_size);
    #ifdef USE_MPI
        MPI_Allgatherv(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, indices_, vec_sizes, disps, MPI_LONG_LONG, MPI_COMM_WORLD);
        MPI_Allgatherv(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, values_.data(), vec_sizes, disps, CONV_MPI(my_mpi_type), MPI_COMM_WORLD);
    #endif
        curr_size_ = tot_size;
    }
};


template <class el_type>
void Adder<el_type>::add(long long idx, el_type val, long long ini_flag) {
    int proc_idx = parent_vec_->idx_to_proc(idx);
    int *count = &send_cts_[proc_idx];
    if (*count == send_idx_.cols()) {
        enlarge_();
    }
    send_idx_(proc_idx, *count) = idx | ini_flag;
    send_vals_(proc_idx, *count) = val;
    (*count)++;
}

template <class el_type>
void Adder<el_type>::perform_add(long long ini_bit) {
    int n_procs = 1;
    int proc_idx;
    
    size_t el_size = sizeof(el_type);
#ifdef USE_MPI
    MPI_Comm_size(MPI_COMM_WORLD, &n_procs);
    MPI_Alltoall(send_cts_, 1, MPI_INT, recv_cts_, 1, MPI_INT, MPI_COMM_WORLD);
    MPI_Alltoallv(send_idx_[0], send_cts_, displacements_, MPI_LONG_LONG, recv_idx_[0], recv_cts_, displacements_, MPI_LONG_LONG, MPI_COMM_WORLD);
    MPI_Alltoallv(send_vals_[0], send_cts_, displacements_, CONV_MPI(my_mpi_type), recv_vals_[0], recv_cts_, displacements_, CONV_MPI(my_mpi_type), MPI_COMM_WORLD);
#else
    for (proc_idx = 0; proc_idx < n_procs; proc_idx++) {
        int cpy_size = send_cts_[proc_idx];
        recv_cts_[proc_idx] = cpy_size;
        memcpy(recv_idx_[proc_idx], send_idx_[proc_idx], cpy_size * sizeof(long long));
        memcpy(recv_vals_[proc_idx], send_vals_[proc_idx], cpy_size * el_size);
    }
#endif
    // Move elements from receiving buffers to vector
    for (proc_idx = 0; proc_idx < n_procs; proc_idx++) {
        send_cts_[proc_idx] = 0;
        parent_vec_->add_elements(recv_idx_[proc_idx], recv_vals_[proc_idx], recv_cts_[proc_idx], ini_bit);
    }
}


#endif /* vec_utils_h */
